import re, json, time, hashlib
from glob import glob
from pathlib import Path
from tqdm import tqdm
import multiprocessing as mp
from itertools import islice
from hanspell import spell_checker  # ë§ì¶¤ë²• ê²€ì‚¬ ë¼ì´ë¸ŒëŸ¬ë¦¬ import ì¶”ê°€

# â”€â”€â”€ í”„ë¡œì íŠ¸ ë£¨íŠ¸ & ë°ì´í„° ë””ë ‰í„°ë¦¬ ì„¤ì • â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€
BASE_DIR   = Path(__file__).resolve().parent.parent
RAW_DIR    = BASE_DIR / "data" / "raw_dataset"
CLEAN_DIR  = BASE_DIR / "data" / "cleaned"
STOPWORDS  = set((BASE_DIR / "scripts" / "korean_stopwords.txt")
                 .read_text(encoding="utf-8").splitlines())

# Okt ì´ˆê¸°í™”ëŠ” worker ë‚´ë¶€ì—ì„œ í•œ ë²ˆë§Œ ìˆ˜í–‰
global_okt = None

def init_worker():
    global global_okt
    from konlpy.tag import Okt
    global_okt = Okt()
    print("Worker initialized with Okt")

def fingerprint(s: str) -> str:
    return hashlib.md5(s.encode("utf-8")).hexdigest()

def clean_line(line: str) -> str:
    # ì†Œë¬¸ìí™”â†’íŠ¹ìˆ˜ë¬¸ì ì œê±°â†’ë§ì¶¤ë²• êµì • ì •ë„ê¹Œì§€ë§Œ ì ìš©
    s = re.sub(r"[^ê°€-í£0-9\s\.\,\?]", " ", line)
    s = re.sub(r"\s+", " ", s).strip()
    # ë¶ˆìš©ì–´(ì˜ë¯¸ ì—†ëŠ” ì¡°ì‚¬/ì–´ë¯¸)ë§Œ ì œê±°
    for sw in STOPWORDS:
        s = s.replace(sw+" ", "")
    return s

    # 2) ë§ì¶¤ë²• êµì • - ë¶„í• í•˜ì—¬ ì²˜ë¦¬ (hanspellì˜ ë¬¸ì ì œí•œ)
    try:
        # hanspellì€ 500ì ì œí•œì´ ìˆì–´ì„œ ì§§ì€ ë‹¨ìœ„ë¡œ ë‚˜ëˆ ì„œ ì²˜ë¦¬
        MAX_SPELL_LEN = 400  # ì—¬ìœ ìˆê²Œ ì„¤ì •
        if len(s) <= MAX_SPELL_LEN:
            checked = spell_checker.check(s)
            s = checked.checked
        else:
            # ê¸´ í…ìŠ¤íŠ¸ëŠ” ì²­í¬ë¡œ ë‚˜ëˆ ì„œ ì²˜ë¦¬
            chunks = [s[i:i+MAX_SPELL_LEN] for i in range(0, len(s), MAX_SPELL_LEN)]
            corrected_chunks = []
            for chunk in chunks:
                try:
                    checked = spell_checker.check(chunk)
                    corrected_chunks.append(checked.checked)
                except Exception as e:
                    print(f"Spell check error for chunk: {e}")
                    corrected_chunks.append(chunk)  # ì˜¤ë¥˜ ì‹œ ì›ë³¸ ì‚¬ìš©
            s = ''.join(corrected_chunks)
    except Exception as e:
        print(f"Spell check error: {e}")
        # ë§ì¶¤ë²• ì˜¤ë¥˜ ì‹œ ì›ë³¸ ì‚¬ìš©

    # 3) í˜•íƒœì†Œ ë¶„ì„ â†’ ì–´ê·¼ ì¶”ì¶œ
    try:
        morphs = global_okt.morphs(s, stem=True)
        toks = [m for m in morphs if m not in STOPWORDS and len(m) > 1]
        return " ".join(toks)
    except Exception as e:
        print(f"Error in morphological analysis: {e}")
        return ""

def process_chunk(chunk):
    results = []
    for line in chunk:
        cleaned = clean_line(line)
        if cleaned:
            results.append(cleaned)
    return results

def process_file(path: Path):
    print(f"\nâ–¶ Processing {path.name}")
    start = time.time()

    # 1) JSONL íŒŒì¼ ì²˜ë¦¬ ë‹¨ê³„ë³„ í™•ì¸
    print("  - íŒŒì¼ ë¡œë“œ ì¤‘...")
    raw_lines = []
    
    # íŒŒì¼ì„ í•œ ì¤„ì”© ì½ì–´ì„œ ì²˜ë¦¬ (ë©”ëª¨ë¦¬ íš¨ìœ¨ì )
    with open(path, 'r', encoding='utf-8') as f:
        for i, line in enumerate(f):
            try:
                obj = json.loads(line)
                raw_lines.extend(obj.get("recommendations", []))
                if i % 1000 == 0:
                    print(f"    * {i}ì¤„ ì²˜ë¦¬ ì™„ë£Œ, í˜„ì¬ {len(raw_lines)}ê°œ ë¬¸ì¥ ì¶”ì¶œ")
            except json.JSONDecodeError:
                print(f"  - JSON íŒŒì‹± ì˜¤ë¥˜ (ì¤„ {i})")
                continue
    
    total = len(raw_lines)
    print(f"  - ì´ {total}ê°œ ë¬¸ì¥ ë¡œë“œ ì™„ë£Œ")
    
    if total == 0:
        print("  - ì²˜ë¦¬í•  ë¬¸ì¥ì´ ì—†ìŠµë‹ˆë‹¤.")
        return

    # 2) ë§ì¶¤ë²• ì²­í¬ ì²˜ë¦¬ ê²½ê³¼ í‘œì‹œ
    print("  - ë³‘ë ¬ ì²˜ë¦¬ ì‹œì‘...")
    print("    * ê° ì‘ì—…ìëŠ” í…ìŠ¤íŠ¸ ì •ì œ, ë§ì¶¤ë²• ê²€ì‚¬, í˜•íƒœì†Œ ë¶„ì„ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.")
    
    CHUNK_SIZE = 100  # ì¡°ì • ê°€ëŠ¥
    chunks = [raw_lines[i:i+CHUNK_SIZE] for i in range(0, len(raw_lines), CHUNK_SIZE)]
    
    cleaned_all = []
    with mp.Pool(min(mp.cpu_count(), 4), initializer=init_worker) as pool:
        for result in tqdm(pool.imap(process_chunk, chunks), total=len(chunks), desc="  cleaning"):
            cleaned_all.extend(result)
            # ì£¼ê¸°ì ìœ¼ë¡œ ì§„í–‰ ìƒí™© ì¶œë ¥
            if len(cleaned_all) % 1000 == 0:
                print(f"    * {len(cleaned_all)}/{total} ì™„ë£Œ")

    # 3) ê¸¸ì´Â·ì¤‘ë³µ í•„í„°
    print("  - ì¤‘ë³µ í•„í„°ë§ ì¤‘...")
    out, seen = [], set()
    for s in cleaned_all:
        if len(s.split()) < 3:
            continue
        h = fingerprint(s)
        if h in seen:
            continue
        seen.add(h)
        out.append(s)

    # 4) ë¹„ì •ìƒ ë¹„ìœ¨ ì²´í¬
    bad_rate = 1 - len(out) / total
    print(f"  - í•„í„°ë§ í›„: {len(out)}ê°œ ë¬¸ì¥ (bad_rate={bad_rate:.2%})")
    if bad_rate >= 0.02:
        print(f"  - ê²½ê³ : ë¹„ì •ìƒ ë¹„ìœ¨ì´ ë†’ìŠµë‹ˆë‹¤: {bad_rate:.2%}")
        # ë¹„ì •ìƒ ë¹„ìœ¨ì´ ë†’ì•„ë„ ê³„ì† ì§„í–‰

    # 5) ì €ì¥
    CLEAN_DIR.mkdir(parents=True, exist_ok=True)
    key = path.stem
    out_path = CLEAN_DIR / f"{key}.txt"
    with open(out_path, 'w', encoding='utf-8') as f:
        f.write("\n".join(out))

    elapsed = time.time() - start
    print(f"âœ… {path.name}: {elapsed:.1f}ì´ˆ ì†Œìš” â†’ {out_path}")

if __name__ == "__main__":
    all_files = list(RAW_DIR.glob("*.jsonl"))
    print(f"Found {len(all_files)} files to process.")
    
    # ë””ë²„ê¹…ì„ ìœ„í•´ í•˜ë‚˜ì˜ íŒŒì¼ë§Œ ì²˜ë¦¬
    # process_file(all_files[0])
    
    # ë§ì¶¤ë²• API ìƒíƒœ í™•ì¸
    print("ë§ì¶¤ë²• ê²€ì‚¬ê¸° ìƒíƒœ í™•ì¸ ì¤‘...")
    try:
        test_result = spell_checker.check("ì•ˆë…•í•˜ì„¸ìš” ë°˜ê°‘ìŠµë‹ˆë‹¤.")
        print(f"ë§ì¶¤ë²• ê²€ì‚¬ê¸° ì •ìƒ ì‘ë™: {test_result.checked}")
    except Exception as e:
        print(f"âš ï¸ ê²½ê³ : ë§ì¶¤ë²• ê²€ì‚¬ê¸° ì˜¤ë¥˜ - {e}")
        print("ë§ì¶¤ë²• ê²€ì‚¬ ì—†ì´ ì§„í–‰í•©ë‹ˆë‹¤.")
    
    # ëª¨ë“  íŒŒì¼ ì²˜ë¦¬
    for p in all_files:
        process_file(p)
    print("\nğŸ‰ All done.")